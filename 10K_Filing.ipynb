{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2667348-92a6-4c1d-b8bf-22e08fac04b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain==0.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aadc8c6-421f-4fe9-bcee-16e437968fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain-community==0.0.10 langchain-openai==0.0.2 langchain-pinecone==0.0.1 sec-edgar-downloader==5.0.0 pypdf==3.17.4 tiktoken==0.5.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d85d52-4b29-4766-be55-f1ddbfb06e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python-dotenv==1.0.0 pinecone-client==3.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00f0685-5df3-415d-a997-d17d25da32d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain_community.document_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301d9308-7e66-4192-ac11-6757cab37b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain_text_splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c11d8b-d628-4982-bbe6-b9d5cec63bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U langchain langchain-community langchain-openai langchain-pinecone langchain-text-splitters sec-edgar-downloader pypdf tiktoken python-dotenv pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceeaad9-2aeb-4663-93b5-299c418c1f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "from sec_edgar_downloader import Downloader\n",
    "from langchain_community.document_loaders import PyPDFLoader, BSHTMLLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def fetch_10k_filings(ticker, email=\"maturamufaro@gmail.com\"):\n",
    "    print(f\"ðŸ“¥ Fetching 10-K for {ticker}...\")\n",
    "    \n",
    "    # Check where we are currently running\n",
    "    current_dir = os.getcwd()\n",
    "    print(f\"ðŸ“‚ Current Working Directory: {current_dir}\")\n",
    "    \n",
    "    dl = Downloader(\"MyRAGPortfolio\", email)\n",
    "    dl.get(\"10-K\", ticker, limit=1)\n",
    "\n",
    "    # 1. Define the base folder based on your specific path\n",
    "    # We look for: current_dir/sec-edgar-filings/AAPL/10-K/\n",
    "    base_search_path = os.path.join(current_dir, \"sec-edgar-filings\", ticker, \"10-K\")\n",
    "    \n",
    "    if not os.path.exists(base_search_path):\n",
    "        print(f\"âŒ Error: The folder {base_search_path} does not exist.\")\n",
    "        return None\n",
    "\n",
    "    # 2. Find the Accession Number folder (e.g., 0000320193-25-000079)\n",
    "    # We take the first folder we find inside '10-K'\n",
    "    subfolders = [f.path for f in os.scandir(base_search_path) if f.is_dir()]\n",
    "    \n",
    "    if not subfolders:\n",
    "        print(f\"âŒ No filing folders found inside {base_search_path}\")\n",
    "        return None\n",
    "    \n",
    "    target_folder = subfolders[0]\n",
    "    print(f\"ðŸ“‚ Found Accession Folder: {target_folder}\")\n",
    "\n",
    "    # 3. Look for the document inside that folder\n",
    "    # Priority: primary-document.html -> primary-document.pdf -> *.txt\n",
    "    possible_files = [\n",
    "        os.path.join(target_folder, \"primary-document.html\"),\n",
    "        os.path.join(target_folder, \"primary-document.htm\"),\n",
    "        os.path.join(target_folder, \"primary-document.pdf\"),\n",
    "        os.path.join(target_folder, \"full-submission.txt\")\n",
    "    ]\n",
    "    \n",
    "    file_path = None\n",
    "    for p in possible_files:\n",
    "        if os.path.exists(p):\n",
    "            file_path = p\n",
    "            break\n",
    "            \n",
    "    # Fallback: Just grab the first file in the directory if standard names fail\n",
    "    if not file_path:\n",
    "        all_files = os.listdir(target_folder)\n",
    "        if all_files:\n",
    "            file_path = os.path.join(target_folder, all_files[0])\n",
    "\n",
    "    if file_path:\n",
    "        print(f\"âœ… Found filing file: {file_path}\")\n",
    "    else:\n",
    "        print(f\"âŒ Folder exists but contains no files: {target_folder}\")\n",
    "        \n",
    "    return file_path\n",
    "\n",
    "def load_document(file_path):\n",
    "    print(\"â³ Loading document... (this might take a moment for large HTML files)\")\n",
    "    if file_path.endswith(\".pdf\"):\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    elif file_path.endswith(\".html\") or file_path.endswith(\".htm\"):\n",
    "        loader = BSHTMLLoader(file_path)\n",
    "    else:\n",
    "        loader = TextLoader(file_path)\n",
    "    return loader.load()\n",
    "\n",
    "def clean_text(text):\n",
    "    import re\n",
    "    # Remove \"Page X of Y\" artifacts\n",
    "    text = re.sub(r\"Page \\d+ of \\d+\", \"\", text)\n",
    "    text = re.sub(r\"Table of Contents\", \"\", text)\n",
    "    # Collapse multiple newlines\n",
    "    text = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", text)\n",
    "    return text\n",
    "\n",
    "def ingest_data():\n",
    "    file_path = fetch_10k_filings(\"AAPL\", \"maturamufaro@gmail.com\")\n",
    "    \n",
    "    if not file_path:\n",
    "        return\n",
    "\n",
    "    # LOAD\n",
    "    try:\n",
    "        raw_documents = load_document(file_path)\n",
    "        print(f\"   Loaded content (approx {len(raw_documents)} pages/sections).\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading file: {e}\")\n",
    "        return\n",
    "\n",
    "    # CHUNK\n",
    "    print(\"âœ‚ï¸  Chunking text...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    documents = text_splitter.split_documents(raw_documents)\n",
    "    \n",
    "    # CLEAN & METADATA\n",
    "    print(\"ðŸ§¹ Cleaning text...\")\n",
    "    for doc in documents:\n",
    "        doc.page_content = clean_text(doc.page_content)\n",
    "        doc.metadata[\"company\"] = \"AAPL\"\n",
    "        doc.metadata[\"year\"] = \"2023\"\n",
    "\n",
    "    print(f\"ðŸ§© Created {len(documents)} chunks.\")\n",
    "    \n",
    "    # EMBED & STORE\n",
    "    if os.getenv(\"OPENAI_API_KEY\") and os.getenv(\"PINECONE_API_KEY\"):\n",
    "        print(\"ðŸš€ Upserting to Pinecone (this takes about 30-60s)...\")\n",
    "        index_name = \"financial-10k\"\n",
    "        \n",
    "        batch_size = 100\n",
    "        for i in range(0, len(documents), batch_size):\n",
    "            batch = documents[i:i+batch_size]\n",
    "            PineconeVectorStore.from_documents(\n",
    "                batch,\n",
    "                OpenAIEmbeddings(),\n",
    "                index_name=index_name\n",
    "            )\n",
    "            print(f\"   Uploaded batch {i // batch_size + 1}\")\n",
    "        print(\"âœ… Ingestion Complete!\")\n",
    "    else:\n",
    "        print(\"ðŸ›‘ API Keys missing. Skipping Pinecone upload.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ingest_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adcd18d-df5f-4b66-bc30-3a784376e3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_classic.chains import create_retrieval_chain\n",
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 1. Load Secrets\n",
    "load_dotenv(override=True)\n",
    "\n",
    "def start_chat():\n",
    "    print(\"ðŸ§  Initializing the Financial Analyst AI...\")\n",
    "\n",
    "    # 2. Connect to the \"Brain\" (LLM)\n",
    "    # We use gpt-4o-mini because it's fast, cheap, and smart enough for this.\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    # 3. Connect to the (Vector DB)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = PineconeVectorStore(\n",
    "        index_name=\"financial-10k\", \n",
    "        embedding=embeddings\n",
    "    )\n",
    "    \n",
    "    # 4. Set up the Retriever\n",
    "    # \"k=3\" means: \"Find the 3 most relevant chunks for every question\"\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "    # 5. Create the System Prompt (The Persona)\n",
    "    # This instructs the AI to ONLY use the retrieved data.\n",
    "    system_prompt = (\n",
    "        \"You are a senior financial analyst. \"\n",
    "        \"Use the provided context to answer the user's question. \"\n",
    "        \"If the answer is not in the context, say 'I could not find information relating to that, sorry'. \"\n",
    "        \"Keep your answers concise and professional.\\n\\n\"\n",
    "        \"Context: {context}\"\n",
    "    )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ])\n",
    "\n",
    "    # 6. Build the Chain\n",
    "    # This connects: Retriever -> Document Combiner -> LLM\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "    rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "    print(\"âœ… Ready! Ask a question about Apple (AAPL). Type 'exit' to quit.\\n\")\n",
    "\n",
    "    # 7. Chat Loop\n",
    "    while True:\n",
    "        query = input(\"User: \")\n",
    "        if query.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "\n",
    "        # Run the RAG pipeline\n",
    "        response = rag_chain.invoke({\"input\": query})\n",
    "        \n",
    "        print(f\"\\nAI: {response['answer']}\")\n",
    "        \n",
    "        # Optional: Show sources (Proof it's working)\n",
    "        print(\"\\n--- Source Documents ---\")\n",
    "        for i, doc in enumerate(response[\"context\"]):\n",
    "            print(f\"Source {i+1}: {doc.page_content[:100]}...\") # Print first 100 chars\n",
    "        print(\"-\" * 30 + \"\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
